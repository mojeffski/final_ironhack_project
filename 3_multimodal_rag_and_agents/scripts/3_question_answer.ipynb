{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.schema import Document\n",
    "\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install -U langchain openai chromadb langchain-experimental # (newest versions required for multi-modal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lock to 0.10.19 due to a persistent bug in more recent versions\n",
    "#! pip install \"unstructured[all-docs]==0.10.19\" pillow pydantic lxml pillow matplotlib tiktoken open_clip_torch torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the parent directory to sys.path\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from utils import preprocessing # functions for preprocessing the pdf texts\n",
    "from utils import txt_gen # functions to generate ai based texts\n",
    "from utils import vectors # functions for creating vector database\n",
    "\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "OPENAI_API_KEY  = os.getenv('OPENAI_API_KEY')\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGSMITH_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"../../1_notification_scraper/NOTIFICATIONS\"\n",
    "law_code = \"2020-0791-F\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the SQLite database\n",
    "db_path = os.path.join(\"..\", \"db\", \"notifications.db\")\n",
    "engine = create_engine(f\"sqlite:///{db_path}\")\n",
    "\n",
    "# Load only the required columns into a DataFrame\n",
    "query = \"\"\"\n",
    "    SELECT tris_id, title, year, country_labels, url, \n",
    "           executive_summary_draft_law, category_labels_broad, category_labels\n",
    "    FROM notifications\n",
    "\"\"\"\n",
    "notifications_md = pd.read_sql_query(query, con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nvectorstore = Chroma(\\n        collection_name=\"test_tris\",\\n        embedding_function=OpenAIEmbeddings(),\\n        persist_directory=os.path.join(\"..\", \"db\")\\n    )\\n\\nvectorstore.persist()\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "vectorstore = Chroma(\n",
    "        collection_name=\"test_tris\",\n",
    "        embedding_function=OpenAIEmbeddings(),\n",
    "        persist_directory=os.path.join(\"..\", \"db\")\n",
    "    )\n",
    "\n",
    "vectorstore.persist()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc(root_dir,law_code,metadata):\n",
    "    #parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "    #sys.path.append(parent_dir)\n",
    "    \n",
    "    #print('os.getcwd()',os.getcwd())\n",
    "    #root_dir = os.getcwd()+'/static/TRIS_DOCUMENTS_FULL'\n",
    "    folder_name = law_code.replace('-','_') \n",
    "    folder_path = os.path.join(root_dir, folder_name)\n",
    "    notification_path = os.path.join(folder_path, 'notification')\n",
    "    \n",
    "            # List of files in the notification folder\n",
    "    files_in_notification = os.listdir(notification_path)\n",
    "     # Identify paths for DRAFT and FINAL files in English\n",
    "    draft_file_path = next(\n",
    "                (os.path.join(notification_path, file) for file in files_in_notification\n",
    "                 if \"DRAFT\" in file and \"EN\" in file and file.lower().endswith(('.pdf', '.docx','.doc'))),\n",
    "                None\n",
    "            )\n",
    "\n",
    "    data = (draft_file_path,metadata)\n",
    "    #print('draft_file_path+meta:',data)\n",
    "    \n",
    "    raw_elements_with_md = preprocessing.extract_elements_with_metadata(data[0],metadata=data[1])\n",
    "    \n",
    "    # Categorize into text and tables\n",
    "    texts, tables = preprocessing.categorize_elements(raw_elements_with_md)\n",
    "    #print('processed texts:',texts)\n",
    "    \n",
    "    #generate image summaries\n",
    "    img_base64_list, image_summaries = txt_gen.generate_img_summaries(data[0])\n",
    "    \n",
    "    # Set up Chroma vector store with persistence\n",
    "    vectorstore = Chroma(\n",
    "        collection_name=\"test_tris\",\n",
    "        embedding_function=OpenAIEmbeddings(),\n",
    "        #persist_directory=os.path.join(\"..\", \"db\")\n",
    "    )\n",
    "\n",
    "    # Persist data after adding documents\n",
    "    #vectorstore.persist()\n",
    "    # Create retriever\n",
    "    text_summaries, table_summaries = txt_gen.generate_text_summaries(\n",
    "        OPENAI_API_KEY,texts, tables, summarize_texts=True\n",
    "    )\n",
    "    retriever_multi_vector_img = vectors.create_multi_vector_retriever(\n",
    "        vectorstore,\n",
    "        text_summaries,\n",
    "        texts,\n",
    "        table_summaries,\n",
    "        tables,\n",
    "        image_summaries,\n",
    "        img_base64_list,\n",
    "    )\n",
    "    return retriever_multi_vector_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(query,retriever_multi_vector_img):\n",
    "    chain_multimodal_rag = txt_gen.multi_modal_rag_chain(retriever_multi_vector_img)\n",
    "    return chain_multimodal_rag.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"]=\"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=\"tris project\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ../../1_notification_scraper/NOTIFICATIONS/2020_0791_F/notification/2020_0791_FDRAFT_N-2020-0791-000-EN.docx\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "partition_docx is not available. Install the docx dependencies with pip install \"unstructured[docx]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m law_code \u001b[38;5;129;01min\u001b[39;00m law_code_list:\n\u001b[1;32m     11\u001b[0m     law_code_md \u001b[38;5;241m=\u001b[39m notifications_md[notifications_md[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtris_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39mlaw_code]\u001b[38;5;241m.\u001b[39mto_dict(orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecords\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 12\u001b[0m     multimodal_retriever \u001b[38;5;241m=\u001b[39m \u001b[43mget_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlaw_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlaw_code_md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     general_question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe question addresses \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlaw_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m     user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease summarize this draft legislation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[29], line 23\u001b[0m, in \u001b[0;36mget_doc\u001b[0;34m(root_dir, law_code, metadata)\u001b[0m\n\u001b[1;32m     20\u001b[0m data \u001b[38;5;241m=\u001b[39m (draft_file_path,metadata)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#print('draft_file_path+meta:',data)\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m raw_elements_with_md \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_elements_with_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Categorize into text and tables\u001b[39;00m\n\u001b[1;32m     26\u001b[0m texts, tables \u001b[38;5;241m=\u001b[39m preprocessing\u001b[38;5;241m.\u001b[39mcategorize_elements(raw_elements_with_md)\n",
      "File \u001b[0;32m~/Documents/Ironhack/Week 7/ironhack_final_project/3_multimodal_rag_and_agents/utils/preprocessing.py:16\u001b[0m, in \u001b[0;36mextract_elements_with_metadata\u001b[0;34m(full_path, metadata)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Partition and extract document elements\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m elements \u001b[38;5;241m=\u001b[39m \u001b[43mpartition\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_page_breaks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextract_images_in_pdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunking_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mby_title\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_characters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_after_n_chars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3800\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#combine_text_under_n_chars=2000,\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_output_dir_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Attach metadata to each element if provided\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/Ironhack/Week 7/ironhack_final_project/venv/lib/python3.13/site-packages/unstructured/partition/auto.py:306\u001b[0m, in \u001b[0;36mpartition\u001b[0;34m(filename, content_type, file, file_filename, url, include_page_breaks, strategy, encoding, paragraph_grouper, headers, skip_infer_table_types, ssl_verify, ocr_languages, languages, detect_language_per_element, pdf_infer_table_structure, pdf_extract_images, pdf_extract_element_types, pdf_image_output_dir_path, pdf_extract_to_payload, xml_keep_tags, data_source_metadata, metadata_filename, request_timeout, hi_res_model_name, model_name, **kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m     elements \u001b[38;5;241m=\u001b[39m _partition_doc(\n\u001b[1;32m    298\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[1;32m    299\u001b[0m         file\u001b[38;5;241m=\u001b[39mfile,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    304\u001b[0m     )\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m filetype \u001b[38;5;241m==\u001b[39m FileType\u001b[38;5;241m.\u001b[39mDOCX:\n\u001b[0;32m--> 306\u001b[0m     _partition_docx \u001b[38;5;241m=\u001b[39m \u001b[43m_get_partition_with_extras\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdocx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m     elements \u001b[38;5;241m=\u001b[39m _partition_docx(\n\u001b[1;32m    308\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[1;32m    309\u001b[0m         file\u001b[38;5;241m=\u001b[39mfile,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    314\u001b[0m     )\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m filetype \u001b[38;5;241m==\u001b[39m FileType\u001b[38;5;241m.\u001b[39mODT:\n",
      "File \u001b[0;32m~/Documents/Ironhack/Week 7/ironhack_final_project/venv/lib/python3.13/site-packages/unstructured/partition/auto.py:114\u001b[0m, in \u001b[0;36m_get_partition_with_extras\u001b[0;34m(doc_type, partition_with_extras_map)\u001b[0m\n\u001b[1;32m    112\u001b[0m _partition_func \u001b[38;5;241m=\u001b[39m partition_with_extras_map\u001b[38;5;241m.\u001b[39mget(doc_type)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _partition_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartition_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not available. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    116\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstall the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dependencies with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    117\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munstructured[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    118\u001b[0m     )\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _partition_func\n",
      "\u001b[0;31mImportError\u001b[0m: partition_docx is not available. Install the docx dependencies with pip install \"unstructured[docx]\""
     ]
    }
   ],
   "source": [
    "law_code_list = [\n",
    "    \"2020-0791-F\",\n",
    "    \"2020-0832-F\",\n",
    "    \"2023-0503-SE\"\n",
    "]\n",
    "\n",
    "evaluation_data = []\n",
    "\n",
    "for law_code in law_code_list:\n",
    "    \n",
    "    law_code_md = notifications_md[notifications_md['tris_id']==law_code].to_dict(orient='records')[0]\n",
    "    multimodal_retriever = get_doc(root_dir,law_code,law_code_md)\n",
    "    general_question = f\"the question addresses {law_code}: \"\n",
    "    user_input = \"please summarize this draft legislation\"\n",
    "    user_input = general_question+user_input\n",
    "    if user_input:\n",
    "        response = chatbot(user_input, multimodal_retriever)\n",
    "        evaluation_data.append({'id':law_code,\n",
    "                              'question':user_input,\n",
    "                              'generated_answer':response,\n",
    "                              'original_summary': law_code_md['executive_summary_draft_law']\n",
    "                              }\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluation metrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def rouge_summary_evaluation(run, example):\n",
    "    \"\"\"\n",
    "    Evaluates generated output against reference using ROUGE metrics for summarization.\n",
    "    Returns an average of ROUGE-1, ROUGE-2, and ROUGE-L F1 scores as a single score.\n",
    "    \"\"\"\n",
    "    predicted_output = run.outputs[\"output\"]\n",
    "    reference_output = example.outputs[\"output\"]\n",
    "\n",
    "    # Initialize ROUGE scorer\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference_output, predicted_output)\n",
    "\n",
    "    # Calculate the average F1 score across ROUGE-1, ROUGE-2, and ROUGE-L\n",
    "    avg_f1_score = (scores['rouge1'].fmeasure + scores['rouge2'].fmeasure + scores['rougeL'].fmeasure) / 3\n",
    "\n",
    "    # Return a single score as a dictionary\n",
    "    return {\"key\": \"rouge_summary_evaluation\", \"score\": avg_f1_score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### wrapper function for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a wrapper function to dynamically create the retriever and call the chatbot\n",
    "def chatbot_with_retriever(input):\n",
    "    # Extract question and law_code from the input\n",
    "    question = input[\"question\"]\n",
    "    law_code = input[\"law_code\"]\n",
    "    print('law code',law_code)\n",
    "    law_code_md = notifications_md[notifications_md['tris_id'] == law_code].to_dict(orient='records')[0]\n",
    "    \n",
    "    #multimodal_retriever for each example\n",
    "    multimodal_retriever = get_doc(root_dir, law_code, law_code_md)\n",
    "\n",
    "    # Get the chatbot response\n",
    "    return {\"output\": chatbot(question, multimodal_retriever)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client, evaluate\n",
    "import os\n",
    "\n",
    "# Initialize Langsmith client\n",
    "client = Client()\n",
    "\n",
    "# Define your test cases\n",
    "dataset_name = \"notification evaluator 1\"\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name, \n",
    "    description=\"Dataset for testing Q&A bot responses on summarizing draft legislation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "# Initialize Langsmith client\n",
    "client = Client()\n",
    "\n",
    "# Define the dataset name\n",
    "dataset_name = \"notification evaluator 1\"\n",
    "\n",
    "# Check if the dataset exists\n",
    "existing_datasets = client.list_datasets()  # Fetch a list of existing datasets\n",
    "dataset = next((ds for ds in existing_datasets if ds.name == dataset_name), None)\n",
    "\n",
    "# If the dataset does not exist, create it; otherwise, use the existing one\n",
    "if dataset is None:\n",
    "    dataset = client.create_dataset(\n",
    "        dataset_name, \n",
    "        description=\"Dataset for testing Q&A bot responses on summarizing draft legislation\"\n",
    "    )\n",
    "else:\n",
    "    print(f\"Using existing dataset: {dataset.name}\")\n",
    "\n",
    "# Now, you can use the dataset object to add examples or perform other operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define law codes to evaluate\n",
    "law_code_list = [\"2020-0791-F\", \"2020-0832-F\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate the dataset with examples\n",
    "for law_code in law_code_list:\n",
    "    # Generate law metadata for the current law code\n",
    "    law_code_md = notifications_md[notifications_md['tris_id'] == law_code].to_dict(orient='records')[0]\n",
    "    question = f\"the question addresses {law_code}: please summarize this draft legislation\"\n",
    "    expected_summary = law_code_md['executive_summary_draft_law']\n",
    "    \n",
    "    #add correct responses and law id to the client\n",
    "    client.create_example(\n",
    "        inputs={\"question\": question, \"law_code\": law_code},\n",
    "        outputs={\"output\": expected_summary},\n",
    "        dataset_id=dataset.id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the parent directory to sys.path\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from utils import preprocessing # functions for preprocessing the pdf texts\n",
    "from utils import txt_gen # functions to generate ai based texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the evaluation\n",
    "experiment_results = evaluate(\n",
    "    lambda input: chatbot_with_retriever(input),\n",
    "    data=dataset_name,\n",
    "    evaluators=[rouge_summary_evaluation],\n",
    "    experiment_prefix=\"qa-bot-experiment\",\n",
    "    metadata={\n",
    "        \"version\": \"1.0.0\",\n",
    "        \"revision_id\": \"qa-testing\"\n",
    "    },\n",
    ")\n",
    "\n",
    "# Print or analyze the experiment results\n",
    "print(\"On Langsmith check the Experiment results on:\", experiment_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
